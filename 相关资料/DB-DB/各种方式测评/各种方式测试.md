

## PgSQL **导出、导入**性能

以 **6000W** 的数据为例（8.2 GB）

**导入**CSV需要  8分钟左右

**导出**CSV需要 4分钟左右（并且支持，复杂查询语句的导出，能解决各种数据类型Mapping 问题。）



## MySQL **导出、导入**性能

MySQL **导入**CSV 目前只有采用 `LOAD DATA FILE` 方案，**6000W** 的数据（8.2GB） 大约 耗时 45分钟。

MySQL **导出** 现在有多种方案，每种方案 各有所长。

1.  使用 MySQL-Shell 工具，**导出6000W** （8.2GB）数据，4~5 分钟（**只能支持全表导出**，不支持查询结果导出，**无法处理** Boolean 数据类型 Mapping 的问题。）
2.  使用 查询语句创建临时表，再使用 MySQL-Shell 工具导出临时表，创建**6000W**  （8.2GB）临时表耗时 43分钟，导出临时表 4-5 分钟，**合计48分钟** [参考](#方案二，创建临时表。)。

    -   临时表**可以解决**将 Boolean 类型转为True或False等问题；
    -   避免了 sed 方案占用内存过高的问题。
3.  使用 `Select Into Outfile` 方案，将数据导出到 MySQL 服务端本地，然后通过 SCP 命令将CSV发送至CDC Manage，供脚本导入到其他数据源，**6000w**（8.2GB）导出到服务端本地耗时 4分钟左右，SCP 发送耗时 3分钟，**合计7分钟** [参考](#MySQL 导出 方案三)。
    -   执行导出命令可以远程执行，但是由于导出的文件在 MySQL 服务端，使用 SCP 获取该文件需要该机器的用户权限。
    -   也可以根据官网建议，将脚本端的地址通过用网络地址映射到 MySQL 服务端的路径中，当然完成地址映射也需要用户权限。



---

从上面的结果可以看出来，PgSQL 无论在导入还是导出，都有着很好的性能，灵活的语句支撑，问题就在于 MYSQL 端，导入的方案唯一，且耗时久。导出这边支持查询语句导出的耗时久，性能好的又不够灵活。

我现在实现的方案是 Pg这边不涉及临时表，MySQL导出的表中包含 Boolean 等需要Mapping 的数据，则会采取**方案二**（6000w数据导出 48分钟），创建临时表来导出。如果MySQL 导出的表中**不包含**需要Mapping 的数据类型，则直接使用方案一（6000W 4~5分钟），类似于TPC-H中的八张表。

而且目前 CDC-Init（DB-DB）会存在一些极端数据不同数据源支持不同的问题，上次跟你提过，后续会在文档中标注；然后对于二进制的数据，无法做到完全一致（做到完全一致需要在sink 端与source 端都建立临时表，并通过查询语句将字符串转为二进制存储为最终结果，这样做起来性能衰减极大，都快赶上使用JDBC。），因此我这边采用的二进制数据，无论PG还是MySQL，导出时都将其转为十六进制字符串写入CSV文件，导入到Sink端后不处理。

MySQL 导出如果使用方案三，稳定、高效、灵活但是需要一个能够传输文件的用户，这种是否可取？如果使用目前的方案（需要Mapping则用方案二，不需要则方案一），遇到Boolean等数据，会很耗时，我们能接受么？二进制类型的数据，我们能否舍弃？如果不能舍弃，耗时太久我们能接受么？







### 方案二，创建临时表。

以 **600W** 数据（800MB）进行使用 CASE 语句进行字段类型的映射

![select as sf1 4min](Imgs-各种方式测试/select as sf1 4min.png)

以 **6000W** 数据（8.2GB）创建临时表，耗时 43 分钟。

![image-20211108111438590](Imgs-各种方式测试/image-20211108111438590.png)



### MySQL 导出 方案三

使用 `Select into file` 导出 CSV 到本地，不支持导出到客户端 

[参考官方文档](https://dev.mysql.com/doc/refman/8.0/en/select-into.html)

[参考stackoverflow](https://stackoverflow.com/questions/2804332/mysql-select-into-outfile-to-a-different-server)

```sql
-- 先查看 支持导入的目录
show variables like '%secure%';

select *
from lineitem_sf10_mysql
into outfile '/usr/local/download/test1.csv'
FIELDS
-- ENCLOSED BY '-'
TERMINATED BY ','
ESCAPED BY '\\'
LINES TERMINATED BY '\n';
```

测试**导出** **600W**  （800MB）数据 通过该方法 只花了19s，下图为**导出** **6000W**（8.2GB）数据耗时 4 分钟。

![image-20211107110937855](Imgs-各种方式测试/image-20211107110937855.png)



通过 SCP 将文件发送至脚本所在主机 ，耗时 3 分钟。

![image-20211107111207172](Imgs-各种方式测试/image-20211107111207172.png)![image-20211107111416531](Imgs-各种方式测试/image-20211107111416531.png)









## Robert 提醒

```sql
你试一下下面的参数调整看会不会有所不同：

-- 修改MySQL的事务管理参数，在完成加载后参数应该复原原来参数值
set autocommit = 0;
-- https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_autocommit
-- 查看当前会话 ID
-- select connection_id() 
-- show variables like 'autocommit';

set unique_checks = 0;
set foreign_key_checks = 0;
set sql_log_bin=0;

-- 临时禁止在一张表上的所有约束
alter table lineitem disable keys;

-- global settings that should be set in my.cnf
flush_time = 300    -- effective value to Windows whose default is 0
-- following variables have default value 4
innodb_read_io_threads = 8
-- 4

innodb_write_io_threads = 8
-- 4

innodb_buffer_pool_size = 1G
-- 134217728

innodb_log_file_size = 100M
-- 50331648

innodb_log_buffer_size = 64M
-- 16777216

-- show variables where Variable_name in ('autocommit','unique_checks','foreign_key_checks','sql_log_bin','innodb_read_io_threads','innodb_write_io_threads','innodb_buffer_pool_size','innodb_log_file_size','innodb_log_buffer_size');
```



![mysql-load](Imgs-各种方式测试/mysql-load.png)




>   把数据文件等分6份，每个子文件有100万行，总共7个文件。简单在命令行同时跑加载跑两次，一次26秒完成，另外一次30秒完成。环境是LXC容器。




```sh
# 导出六个 CSV
-- SELECT * FROM `lineitem_mysql` LIMIT 1 OFFSET 1;
## for i in {0..5}; do a=$[10000*100];b=$[i*a]; echo "${a} ${b}"; done

for i in {0..5}; do a=$[10000*100];b=$[i*a]; psql postgres://postgres:postgres@192.168.30.155/test_db -c "\copy (select * from lineitem_sf1_pgsql limit $a offset $b) to '/usr/local/lineitem_600w_$i.csv' with DELIMITER ',';"; done
```

 

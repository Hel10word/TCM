##############################################################  Source DatabaseConfig (Required)
## Optional Value:mysql、postgresql
#

# sourceDatabaseName=test_db
# sourceDataType=mysql
# sourceHost=192.168.30.38
# sourcePort=3306
# sourceUser=root
# sourcePassword=root
# sourceTableName=lineitem_test

sourceDatabaseName=test_db
sourceDataType=postgresql
sourceHost=192.168.30.38
sourcePort=5432
sourceUser=root
sourcePassword=root
sourceTableName=lineitem_sf10


##############################################################  Clone Config (Required)

# cloneDatabaseName=test_cdc_hudi
# # Optional Value:mysql、postgresql、hudi
# cloneDataType=hudi
# cloneHost=192.168.120.67
# clonePort=10000
# cloneUser=rapids
# clonePassword=rapids
# cloneTableName=lineitem_demo

#cloneDatabaseName=test_db
#cloneDataType=postgresql
#cloneHost=192.168.30.155
#clonePort=5432
#cloneUser=postgres
#clonePassword=
#cloneTableName=numeric_types_mysql_clone

cloneDatabaseName=test_db
# Optional Value:mysql、postgresql
cloneDataType=mysql
cloneHost=192.168.120.69
clonePort=3306
cloneUser=root
clonePassword=rdpuser
cloneTableName=lineitem_sf10


##############################################################  Hudi Config

# CSV file save to HDFS Path.
# default: N/A (Required)
hdfsSourceDataPath=hdfs:///HudiTest/
# hudi table data save to HDFS Path.
# default: N/A (Required)
hdfsCloneDataPath=hdfs:///HudiTest/demo_Hudi
# hudi Record key field. Normally the sourceTable primary key or UUID.
# default: N/A (Required)
primaryKey=id
# hudi Partition path field. Value to be used at the partitionPath component of HoodieKey.
# default: _hoodie_date (Optional)
partitionKey=_hoodie_date
# hudi Type of table to write.
# Optional Value:COPY_ON_WRITE、MERGE_ON_READ
# default: MERGE_ON_READ (Optional)
hudiTableType=MERGE_ON_READ
# the hudi table should or should not be multiPartitioned.
# whether have hive partition field, if false,default use time field to partition
# default: false (Optional)
nonPartition=false
# the hudi table should or should not be multiPartitioned.
# whether to support hive multi-partition field
# default: false (Optional)
multiPartitionKeys=false


##############################################################  Spark Startup Config
# Custom start command,If you set it, it has the highest priority.
# default: N/A (Required)
# spark 2,hudi 0.8.
sparkCustomCommand=spark-shell \
--jars /opt/CDC/spark/jars/hudi-spark-bundle_2.11-0.8.0.jar \
--driver-class-path $HADOOP_CONF_DIR \
--packages org.apache.spark:spark-avro_2.11:2.4.4 \
--master local[2] \
--deploy-mode client \
--driver-memory 2G \
--executor-memory 2G \
--num-executors 2
# --executor-cores 4
# --conf spark.default.parallelism=60


# spark 3,hudi 0.9
#spark.custom.command = spark-shell \
#--jars /usr/local/spark/spark-3.1.2-bin-hadoop3.2/jars/hudi-spark3-bundle_2.12-0.9.0.jar  \
#--packages org.apache.spark:spark-avro_2.12:3.0.1 \
#--conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
# sf10
#--master spark://192.168.30.221:7078 \
#--conf spark.dymaicAllcation.enabled=true \
#--conf spark.default.parallelism=100 \


##############################################################  TCM Config


# 使用工具 记录 source 端 表的 sql 语句，(默认为 falase)，sql 文件保存在 tempDirectory 目录下。
#getSourceTableSQL=true
# 使用工具 记录 clone 端 表的 sql 语句，(默认为 falase)，sql 文件保存在 tempDirectory 目录下。
#getCloneTableSQL=true
# 是否在 clone 端创建 clone table，(默认为 true)，若不创建 clone table，需要根据 cloneTable 指定一个存在的表。
#createTableInClone=false
# 是否执行 Export 数据的脚本，(默认为 true)
# executeExportScript=false
# 是否执行 Import 数据的脚本，(默认为 true)
# executeLoadScript=false
# 导入导出过程中需要 CSV 文件名，(默认为  "Export_from_" + sourceType + "_" + table.getTableName() + ".csv"; )
# csvFileName=""



# against kafka and hadoop source,should
# sourceSchemaFilePath:./customTable.json
# delete cache file
# default: true (Optional)
deleteCache=false
# The TCM tools generated temp data directory.
# default: ./TCM-Temp (Optional)
tempDirectory=./TCM-Temp
# Separator for each field of CSV file
# default: | (Optional)
delimiter=|
# Whether to output detailed information during the running process
# default: false (Optional)
# debug=true
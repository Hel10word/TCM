sourceDatabaseName=test_db
# mysql、postgresql
sourceDataType=mysql
sourceHost=192.168.120.68
sourcePort=3306
sourceUser=root
sourcePassword=MyNewPass4!
sourceTable=lineitem

# sourceDatabaseName=test_db
# sourceDataType=postgresql
# sourceHost=192.168.120.66
# sourcePort=5432
# sourceUser=root
# sourcePassword=123456
# sourceTable=lineitem_clone

# cloneDatabaseName=test_db
# cloneDataType=postgresql
# cloneHost=192.168.120.66
# clonePort=5432
# cloneUser=root
# clonePassword=123456
# cloneTable=lineitem_clone

cloneDatabaseName=test_cdc_hudi
# mysql、postgresql、util
cloneDataType=hudi
cloneHost=192.168.120.67
clonePort=10000
cloneUser=rapids
clonePassword=rapids
cloneTable=lineitem_cow




# CSV 文件保存到 HDFS 中的目录 (必填)
hdfs.source.data.path=hdfs:///HudiTest/

# util 表数据存储的 HDFS 路径 (必填)
hdfs.clone.data.path=/HudiTest/lineitem_cow

# source表的主键(必填)
primary.key=id

# util 分区键 (默认为 _hoodie_date)
partition.key=_hoodie_date


# util 写入表的类型:COPY_ON_WRITE、MERGE_ON_READ  (默认为 MERGE_ON_READ)
hudi.table.type=COPY_ON_WRITE



# 启动 spark 的命令。 (必填)
spark.custom.command = spark-shell \
--jars /opt/CDC/spark/jars/hudi-spark-bundle_2.11-0.8.0.jar \
--driver-class-path $HADOOP_CONF_DIR \
--packages org.apache.spark:spark-avro_2.11:2.4.4 \
--master yarn --deploy-mode client \
--driver-memory 4g \
--driver-cores 1 \
--num-executors 10 \
--executor-memory 4g \
--executor-cores 2 \
--conf spark.default.parallelism=40 







# 当前任务生成临时文件的目录 (默认为 ‘./TCM-Temp’)
tempDirectory=./TCM-Temp
# CSV 文件每个字段的分隔符 (默认为 '|'')
delimiter=|
# 是否打印每一步的详细输出 (默认为 'false')
debug=true

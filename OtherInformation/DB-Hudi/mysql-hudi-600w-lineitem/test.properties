sourceDatabaseName=test_db
# mysql、postgresql
sourceDataType=mysql
sourceHost=192.168.120.68
sourcePort=3306
sourceUser=root
sourcePassword=MyNewPass4!
sourceTable=lineitem

# sourceDatabaseName=test_db
# sourceDataType=postgresql
# sourceHost=192.168.120.66
# sourcePort=5432
# sourceUser=root
# sourcePassword=123456
# sourceTable=lineitem_clone

# cloneDatabaseName=test_db
# cloneDataType=postgresql
# cloneHost=192.168.120.66
# clonePort=5432
# cloneUser=root
# clonePassword=123456
# cloneTable=lineitem_clone

cloneDatabaseName=test_cdc_hudi
# mysql、postgresql、hudi
cloneDataType=hudi
cloneHost=192.168.120.67
clonePort=10000
cloneUser=rapids
clonePassword=rapids
cloneTable=lineitem_mor


# 当前任务生成临时文件的目录 (默认为 ‘./TCM-Temp’)
# tempDirectory=/data/cdc_data/fabric-cdc/init/TableCloneManageCache/
tempDirectory=./TCM-Temp

# CSV 文件保存到 HDFS 中的目录 (必填)
# hdfs://cdc/
hdfs.source.data.path=hdfs:///HudiTest/

# hudi HDFS 路径 (必填)
# hudi.hdfs.path=/HudiTest/lineitem_test_mor
hdfs.clone.data.path=/HudiTest/lineitem_mor

# hudi 主键 (sourceTable 表的主键,必填)
# hudi.primary.key=id
primary.key=id

# hudi 分区键 (默认为 ‘_hoodie_date’)
# hudi.partition.key=_hoodie_date
partition.key=_hoodie_date


# hive 表类型 (默认为 ‘MERGE_ON_READ’) 可以选为 COPY_ON_WRITE
# hive.table.type=MERGE_ON_READ
hudi.table.type=MERGE_ON_READ

# hive 是否没有分区 (默认为 ‘false’)
# hive.non.partitioned=false
# hive 是否多分区 (默认为 ‘false’)
# hive.multi.partition.keys=false
# 并行度决定了数据集中文件的初始数量 (默认为 '8')
# parallel=8

# CSV 文件每个字段的分隔符 (默认为 '|'')
delimiter=|
# 是否打印每一步的详细输出 (默认为 'false')
debug=true

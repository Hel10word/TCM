sourceDatabaseName=test_db
# mysql、postgresql
sourceDataType=mysql
sourceHost=192.168.30.148
sourcePort=3306
sourceUser=root
sourcePassword=root
sourceTable=lineitem_sf10_mysql

# sourceDatabaseName=test_db
# sourceDataType=postgresql
# sourceHost=192.168.30.148
# sourcePort=5432
# sourceUser=root
# sourcePassword=123456
# sourceTable=lineitem_clone

# cloneDatabaseName=test_db
# cloneDataType=postgresql
# cloneHost=192.168.120.66
# clonePort=5432
# cloneUser=root
# clonePassword=123456
# cloneTable=lineitem_clone

cloneDatabaseName=test_db
# mysql、postgresql、hudi
cloneDataType=hudi
cloneHost=192.168.30.39
clonePort=10000
cloneUser=
clonePassword=
cloneTable=lineitem_sf10_mysql_cow




# CSV 文件保存到 HDFS 中的目录 (必填)
hdfs.source.data.path=hdfs:///HudiTest/

# hudi 表数据存储的 HDFS 路径 (必填)
hdfs.clone.data.path=/HudiTest/lineitem_sf10_mysql_cow

# source表的主键(必填)
primary.key=id

# hudi 分区键 (默认为 _hoodie_date)
partition.key=_hoodie_date


# hudi 写入表的类型:COPY_ON_WRITE、MERGE_ON_READ  (默认为 MERGE_ON_READ)
hudi.table.type=COPY_ON_WRITE



# 启动 spark 的命令。 (必填)
spark.custom.command = spark-shell \
--jars /usr/local/spark/spark-3.1.2-bin-hadoop3.2/jars/hudi-spark3-bundle_2.12-0.9.0.jar  \
--packages org.apache.spark:spark-avro_2.12:3.0.1 \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
--master spark://192.168.30.221:7078 \
--conf spark.dymaicAllcation.enabled=true \
--conf spark.default.parallelism=100







# 当前任务生成临时文件的目录 (默认为 ‘./TCM-Temp’)
tempDirectory=./TCM-Temp
# CSV 文件每个字段的分隔符 (默认为 '|'')
delimiter=|
# 是否打印每一步的详细输出 (默认为 'false')
debug=true

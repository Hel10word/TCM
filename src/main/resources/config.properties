sourceDatabaseName=test_db
# mysql、postgresql
sourceDataType=mysql
sourceHost=192.168.120.68
sourcePort=3306
sourceUser=root
sourcePassword=MyNewPass4!
sourceTable=lineitem_test

#cloneDatabaseName=test_db
#cloneDataType=mysql
#cloneHost=192.168.30.148
#clonePort=3306
#cloneUser=root
#clonePassword=root
#cloneTable=data_time_types_mysql_clone

#cloneDatabaseName=test_db
#cloneDataType=postgresql
#cloneHost=192.168.30.155
#clonePort=5432
#cloneUser=postgres
#clonePassword=
#cloneTable=numeric_types_mysql_clone

cloneDatabaseName=test_cdc_hudi
# mysql、postgresql、hudi
cloneDataType=hudi
cloneHost=192.168.120.67
clonePort=10000
cloneUser=rapids
clonePassword=rapids
cloneTable=lineitem_demo


# 当前任务生成临时文件的目录 (默认为 ‘./TCM-Temp’)
tempDirectory=./TCM-Temp


# CSV 文件保存到 HDFS 中的目录 (必填)
hdfs.source.data.path=hdfs:///HudiTest/
# hudi 表数据的存储路径
hdfs.clone.data.path=hdfs:///HudiTest/demo_Hudi
# hudi 主键 (sourceTable 表的主键,必填)
primary.key=L_COMMENT
# hudi 分区键 (默认为 ‘_hoodie_date’)
partition.key=_hoodie_date
# 生成 hudi 表的类型 (COPY_ON_WRITE、MERGE_ON_READ)
hudi.table.type=MERGE_ON_READ
# hive 是否有分区 (默认为 ‘false’)
#hive.non.partitioned=false
# hive 是否多分区 (默认为 ‘false’)
#hive.multi.partition.keys=false
# 并行度决定了数据集中文件的初始数量 (默认为 ’8‘)
#parallel=8

# CSV 文件每个字段的分隔符 (默认为 ’|‘)
delimiter=|
# 是否打印每一步的详细输出 (默认为 'false')
debug=true

sourceDatabaseName=test_db
# mysql、postgresql
sourceDataType=mysql
sourceHost=192.168.120.68
sourcePort=3306
sourceUser=root
sourcePassword=MyNewPass4!
sourceTable=lineitem

#cloneDatabaseName=test_db
#cloneDataType=mysql
#cloneHost=192.168.30.148
#clonePort=3306
#cloneUser=root
#clonePassword=root
#cloneTable=data_time_types_mysql_clone

#cloneDatabaseName=test_db
#cloneDataType=postgresql
#cloneHost=192.168.30.155
#clonePort=5432
#cloneUser=postgres
#clonePassword=
#cloneTable=numeric_types_mysql_clone

cloneDatabaseName=test_cdc_hudi
# mysql、postgresql、hudi
cloneDataType=hudi
cloneHost=192.168.120.67
clonePort=10000
cloneUser=rapids
clonePassword=rapids
cloneTable=lineitem_demo


# 当前任务生成临时文件的目录 (默认为 ‘./TCM-Temp’)
tempDirectory=./TCM-Temp


# CSV 文件保存到 HDFS 中的目录 (必填)
hdfs.csv.dir=hdfs://HudiTest/
# hudi HDFS 路径 (必填)
hudi.hdfs.path=hdfs://HudiTest/demo_Hudi
# hudi 主键 (sourceTable 表的主键,必填)
hudi.primary.key=L_COMMENT
# hudi 分区键 (默认为 ‘_hoodie_date’)
hudi.partition.key=_hoodie_date
# hive 表类型 (默认为 ‘MERGE_ON_READ’)
hive.table.type=MERGE_ON_READ
# hive 是否有分区 (默认为 ‘false’)
hive.non.partitioned=false
# hive 是否多分区 (默认为 ‘false’)
hive.multi.partition.keys=false
# 并行度决定了数据集中文件的初始数量 (默认为 ’8‘)
parallel=8
# CSV 文件每个字段的分隔符 (默认为 ’|‘)
delimiter=,
# 是否打印每一步的详细输出 (默认为 'false')
debug=true
